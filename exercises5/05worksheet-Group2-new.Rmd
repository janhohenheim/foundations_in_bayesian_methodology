---
title: "Worksheet 05 Group 2"
author:
- "Andrea Staub"
- "Emanuel Mauch"
- "Holly Vuarnoz"
- "Jan Hohenheim"
- "Sophie Haldemann"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# Set chunk options here 
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE}
library(glue)
library(rjags)
library(INLA)
library(MASS)
library(ggplot2)
library(patchwork)
library(coda)
library(stableGR)
source("05ess.r")
```


# Exercise 3
Run the code provided in the ﬁle 05normal_example_JAGS.R and explore the interfaces in R to JAGS. Comment your ﬁndings. -> The comments can be found below the code chunks.



```{r}
###################
# Normal Example (Gibbs MCMC)
###################

remove(list=ls())
set.seed(44566)

# set the path to the 05normal_example_JAGS.txt file
path <- ""
```

clearing environment, `set.seed` for reproducibility and set path to .txt file with model

### Interface in R to JAGS: rjags

```{r}
##########################################################
# rjags interface to JAGS
##########################################################

library(rjags)

list.factories(type = "rng")
list.factories(type = "monitor")

list.factories(type = "sampler")
set.factory(name = "base::Slice", type = "sampler", state = FALSE)
list.factories(type = "sampler")
set.factory(name = "base::Slice", type = "sampler", state = TRUE)
list.factories(type = "sampler")

list.modules()
load.module("glm")
list.modules()
unload.module("glm")
list.modules()
```
With `list.factories` a data frame with two columns is returned, the first column shows the names of the factory objects in the currently loaded modules, and the second column is a logical vector indicating whether the corresponding factory is active or not.
With `list.modules()` the loaded modules can be listed. With `load.module("module_name")` and `unload.module("module_name")` modules can be loaded or unloaded.


```{r}
####################
## Introduction
####################


# generating data
#mu <- 4
#sigma2 <- 16
#n <- 30
#y <- rnorm(n=n, mean=mu, sd=sqrt(sigma2))


# Load the data
# round(y,3)
y <- c(3.048,2.980,2.029,7.249,-0.259,3.061,4.059,6.370,7.902,1.926,
       9.094,10.489,-0.384,-3.096,2.315,5.830,-1.542,-1.544,5.714,
       -5.182,3.828,-4.038,2.169,5.087,-0.201,4.880,3.302,3.859,
       11.144,5.564)

par(mfrow = c(1, 1))
boxplot(y)
summary(y)
sd(y)

# Define the parameters of the prior distributions 
mu0 <- -3
sigma2_0 <- 4
a0 <- 1.6
b0 <- 0.4
```

30 random datapoints were generated from a normal distribution with mean = 4 and standard deviation = 4. The parameters of the prior distribution are defined. Mean value from summary is with `r summary(y)[[4]]` not that close to mean = 4.

```{r}
################################
# INLA exact result (motivation)
################################

# https://www.r-inla.org/download-install
library(INLA)

formula <- y ~ 1
inla.output <- inla(formula,data=data.frame(y=y),
                    control.family = list(hyper =
                                            list(prec = list(prior="loggamma",param=c(a0,b0)))),
                    control.fixed = list(mean.intercept=mu0, prec.intercept=1/sigma2_0))


```

As input for INLA the previously defined parameters and the random sampled datapoints are used. INLA is used to approximate distribution.

```{r}
#############################
# Step 2: JAGS model file as a string in rjags with coda
#############################
set.seed(44566)

library(rjags)
library(coda)


#sessionInfo()

wb_data <- list( N=30, 
                 y=c(3.048,2.980,2.029,7.249,-0.259,3.061,4.059,6.370,7.902,1.926,
                     9.094,10.489,-0.384,-3.096,2.315,5.830,-1.542,-1.544,5.714,
                     -5.182,3.828,-4.038,2.169,5.087,-0.201,4.880,3.302,3.859,
                     11.144,5.564) 
)

wb_inits <- list( mu=-0.2381084, inv_sigma2=0.3993192 )

modelString = " # open quote for modelString
model{
# likelihood
for (i in 1:N){ 
y[i] ~ dnorm( mu, inv_sigma2 )    
}
# Priors
mu ~ dnorm( -3, 0.25 ) # prior for mu N(mu0, prec=1/sigma2_0)
inv_sigma2 ~ dgamma( 1.6, 0.4 ) # prior for precision G(a0, b0)

# transformations
# deterministic definition of variance
sigma2 <- 1/inv_sigma2

# deterministic definition of standard deviation
sigma <- sqrt(sigma2)
}
" # close quote for modelString

writeLines(modelString, con="TempModel.txt") # write to a file

```

The input for JAGS is defined as a list. The model is printed to a new .txt file. N is set to 30 and N samples are drawn from a normal distribution with mean mu and the precision as standard deviation. Mu follows a normal prior and the precision a gamma prior. The variance and then the standard deviation are estimated from the precision. 



```{r}
##########################################################
# JAGS only one chain
##########################################################

# model initiation
model.jags <- jags.model(
  file = "TempModel.txt", 
  data = wb_data,
  inits = wb_inits,
  n.chains = 1,
  n.adapt = 4000
)

str(model.jags)
class(model.jags)
attributes(model.jags)
list.samplers(model.jags)
```

A jags model with only one chain is initiated. The previously generated TempModel.txt file is used. The number of iterations for adaption is set to 4000.



```{r}
# burn-in

update(model.jags, n.iter = 4000)

# sampling
fit.jags.coda <- coda.samples(
  model = model.jags, 
  variable.names = c("mu", "sigma2", "inv_sigma2"), 
  n.iter = 10000,
  thin = 1
)

str(fit.jags.coda)
class(fit.jags.coda)
attributes(fit.jags.coda)

summary(fit.jags.coda)
#print(fit.jags.coda)
plot(fit.jags.coda)


# store samples for each parameter from the chain into separate objects
m.fit.jags.coda <- as.matrix(fit.jags.coda)
mu.sim <- m.fit.jags.coda[,"mu"] 
sigma2.sim <- m.fit.jags.coda[,"sigma2"]
inv_sigma2.sim <- m.fit.jags.coda[,"inv_sigma2"] 

library(MASS)

par(mfrow=c(1,1))
# plot for mean
rg <- range(inla.output$marginals.fixed$"(Intercept)"[,2])
truehist(mu.sim, prob=TRUE, col="yellow", xlab=expression(mu),ylim=rg)
lines(density(mu.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.fixed$"(Intercept)",lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),cex=1.0,bty="n")

# plot for variance
m_var <-inla.tmarginal(function(x) 1/x, inla.output$marginals.hyperpar[[1]])
rg <- range(m_var[,2])
truehist(sigma2.sim, prob=TRUE, col="yellow", xlab=expression(sigma^2),ylim=rg)
lines(density(sigma2.sim),lty=3,lwd=3, col=2)
lines(m_var,lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),cex=1.0,bty="n")

# plot for precision
truehist(inv_sigma2.sim, prob=TRUE, col="yellow", xlab=expression(1/sigma^2))
lines(density(inv_sigma2.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.hyperpar[[1]],lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),cex=1.0,bty="n")
```

The burn in period is set to 4000. After the burn in 10000 iterations are done to fit the model. The traceplots look good and stable. 
When looking at the traceplots we can see that the approximations for mu and sigma^2 from JAGS and INLA look similar but differ just a little bit. 



```{r}
##########################################################
# JAGS several chains
##########################################################

wb_inits <- function() {
  list(mu = rnorm(1),
       inv_sigma2 = runif(1)
  )  
}

# model initialisation
model.jags <- jags.model(
  file = "TempModel.txt", 
  data = wb_data,
  inits = wb_inits,
  n.chains = 4,
  n.adapt = 4000
)

# burn-in

update(model.jags, n.iter = 4000)

# sampling/monitoring
fit.jags.coda <- coda.samples(
  model = model.jags, 
  variable.names = c("mu", "sigma2", "inv_sigma2"), 
  n.iter = 10000,
  thin = 10
)

#n.thin<-floor((n.iter-n.adapt)/500)
#floor((10000-4000)/500)=12

```

Now again JAGS is used with the same model but instead of only one chain, 4 chains are used instead. The burn in period is again 4000.


```{r}
summary(fit.jags.coda)
plot(fit.jags.coda)



# store samples for each parameter from the chains into separate vectors
m.fit.jags.coda <-as.matrix(fit.jags.coda)
mu.sim <- m.fit.jags.coda[,"mu"] 
sigma2.sim <- m.fit.jags.coda[,"sigma2"]
inv_sigma2.sim <- m.fit.jags.coda[,"inv_sigma2"] 


par(mfrow=c(1,1))
# plot for mean
rg <- range(inla.output$marginals.fixed$"(Intercept)"[,2])
truehist(mu.sim, prob=TRUE, col="yellow", xlab=expression(mu),ylim=rg)
lines(density(mu.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.fixed$"(Intercept)",lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),cex=1.0,bty="n")

# plot for variance
m_var <-inla.tmarginal(function(x) 1/x, inla.output$marginals.hyperpar[[1]])
rg <- range(m_var[,2])
truehist(sigma2.sim, prob=TRUE, col="yellow", xlab=expression(sigma^2),ylim=rg)
lines(density(sigma2.sim),lty=3,lwd=3, col=2)
lines(m_var,lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),cex=1.0,bty="n")

# plot for precision
truehist(inv_sigma2.sim, prob=TRUE, col="yellow", xlab=expression(1/sigma^2))
lines(density(inv_sigma2.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.hyperpar[[1]],lwd=2)
legend("topright",c("MCMC: JAGS","INLA"),lty=c(3,1),lwd=c(2,2),col=c(2,1),cex=1.0,bty="n")


## CODA
#summary(fit.jags.coda)
#effectiveSize(fit.jags.coda)
#lapply(fit.jags.coda, effectiveSize)
#gelman.diag(fit.jags.coda,autoburnin=TRUE)
#gelman.plot(fit.jags.coda,autoburnin=TRUE)
#geweke.diag(fit.jags.coda)
#geweke.plot(fit.jags.coda)
#heidel.diag(fit.jags.coda)
#raftery.diag(fit.jags.coda)
#coda:::traceplot(fit.jags.coda)


# "DIC" penalised expected deviance computation
dic1<-dic.samples(model=model.jags, n.iter=1000, type="popt")
#dic2<-dic.samples(model=model.jags2, n.iter=1000, type="popt")

# "DIC" penalised expected deviance comparison
# There is no absolute scale for DIC comparison
# SE is very helpful

#diffdic(dic1,dic2)
```

The traceplots of the four chains are superimposed on each other. They look alright. Also this time the results from approximation with JAGS and INLA look quite similar.



### Interface in R to JAGS: runjags

```{r}

######
# Additional sampling in several chains, preparation for BGR/Gelman
# with runjags
######

library(runjags)

###############
# runjags interface with a link to a file
###############


wb_data <- list( N=30, 
                 y=c(3.048,2.980,2.029,7.249,-0.259,3.061,4.059,6.370,7.902,1.926,
                     9.094,10.489,-0.384,-3.096,2.315,5.830,-1.542,-1.544,5.714,
                     -5.182,3.828,-4.038,2.169,5.087,-0.201,4.880,3.302,3.859,
                     11.144,5.564) 
)

wb_inits <- function() {
  list(mu = rnorm(1),
       inv_sigma2 = runif(1)
  )  
}

fit.runjags<-run.jags(model=paste(path,"05normal_exmple_JAGS.txt",sep=""),
                      monitor=c("mu", "sigma2", "inv_sigma2"),
                      data=wb_data,
                      inits=wb_inits,
                      n.chains=4,
                      burnin=4000,
                      sample=5000,
                      adapt=1000,
                      thin=2)


plot(fit.runjags)
print(fit.runjags)

# CODA
fit.runjags.coda<-as.mcmc.list(fit.runjags)
summary(fit.runjags.coda)
# conduct CODA


```

Another interface is runjags. Now the .txt file 05normal_exmple_JAGS.txt is used where the model is specified. Four chains are used the burnin period is again set to 4000 afterwards 5000 additional samples are taken. In the burnin the adaptive iterations to use for the simulation are not included. Thinning is set to 2. Here burnin is directly specified in the model instead of using `update()`. The traceplots look ok. We also get an empirical cumulative distribution function plot and an autocorrelation plot for the parameters and an additional plot showing the correlations between the parameters. The autocorrelation plots look good for all parameters.



### Interface in R to JAGS: R2jags

```{r}
##########################################################
# R2jags wrapper to rjags interface to JAGS several chains
##########################################################



library(R2jags)

#rm(list=ls())

```

A third interface is R2jags.

```{r}
###############
# R2jags wrapper with a link to a file
###############


wb_data <- list( N=30, 
                 y=c(3.048,2.980,2.029,7.249,-0.259,3.061,4.059,6.370,7.902,1.926,
                     9.094,10.489,-0.384,-3.096,2.315,5.830,-1.542,-1.544,5.714,
                     -5.182,3.828,-4.038,2.169,5.087,-0.201,4.880,3.302,3.859,
                     11.144,5.564) 
)

#define parameters
params<-c("mu", "sigma2", "inv_sigma2")

# define inits
inits1 <- list(mu=rnorm(1), inv_sigma2=runif(1),
               .RNG.name="base::Super-Duper", .RNG.seed=1)
inits2 <- list(mu=rnorm(1), inv_sigma2=runif(1),
               .RNG.name="base::Wichmann-Hill", .RNG.seed=2)
wb_inits <- list(inits1,inits2)


fit.R2jags<-jags(data=wb_data,
                 inits=wb_inits,
                 parameters.to.save=params,
                 model.file=paste(path,"05normal_exmple_JAGS.txt",sep=""),
                 n.chains=2,
                 n.iter=50000,
                 n.burnin=4000,
                 n.thin=5,
                 DIC = TRUE,
                 jags.seed = 321,
                 refresh =100,
                 digits = 4,
                 jags.module = c("glm","dic"))


# Standard plots of the monitored variables
plot(fit.R2jags)

# Display summary statistics
print(fit.R2jags)

# traceplot
traceplot(fit.R2jags)


# CODA
fit.R2jags.coda<-as.mcmc(fit.R2jags)
summary(fit.R2jags.coda)
# conduct CODA
```

Now 2 Markov chains are used and a burnin of 4000. This time `n.iter` = 50000 includes the burnin period. The thinning is set to 5 to save memory and computation time. Also the model is called from the 05normal_exmple_JAGS.txt file. Again burnin is set directly instead of updating the model. Also this time the traceplots look alright.




\newpage

# Exercise 4

Extend the code available in the file 05normal_example_JAGS.R to deal with the logistic
regression example for mice data from Collett (2003, p.71) provided in Table 1.


Compare the output provided by the classic logistic regression and the Bayesian inference. What are the differences?

\hrulefill

\bigskip

```{r}
remove(list=ls())
rm(.Random.seed, envir=globalenv())
set.seed(44566)

# Load data
y <- c(26,9,21,9,6,1)
n <- c(28,12,40,40,40,40)
x <- c(0.0028, 0.0028, 0.0056, 0.0112, 0.0225, 0.0450)

# Priors
mu0 <- 0
prec_0 <- 1.0E-04
```

### INLA 

```{r}
formula <- y ~ 1 + x

inla.output <- inla(formula,
                    data = data.frame(y = y, x = x-mean(x)),
                    family="binomial",
                    Ntrials = n,
                    control.fixed = list(mean = list(intercept = 0, x = 0),
                                         prec = list(intercept = prec_0, x = prec_0)),
                    control.predictor = list(compute = T, link= 1))
```

### JAGS: one chain

```{r results = "hold"}

mice_data <- list(Y = y, x = x - mean(x), n = n)

inits_1chain <- list(a = 0, b = 0, .RNG.name = "base::Wichmann-Hill", .RNG.seed = 123456)

modelString = " # open quote for modelString
model {
# likelihood
for(i in 1:length(Y)) {
  Y[i] ~ dbin(p[i], n[i])
  logit(p[i]) <- a+b*x[i]
}
# priors
a ~ dnorm(0, 1.0E-4)
b ~ dnorm(0, 1.0E-4)
}
" # close quote for modelString

writeLines(modelString, con="MiceModel.txt") # write to a file

# model initiation
model.jags <- jags.model(
  file = "MiceModel.txt",
  data = mice_data,
  inits = inits_1chain,
  n.chains = 1,
  n.adapt = 4000
)

# burn-in
update(model.jags, n.iter = 4000)

# sampling
fit.jags.coda <- coda.samples(
  model = model.jags,
  variable.names = c("a", "b"),
  n.iter = 10000,
  thin = 10
)

#summary(fit.jags.coda)
plot(fit.jags.coda)

# store samples for each parameter from the chain into separate objects
m.fit.jags.coda <- as.matrix(fit.jags.coda)
a.sim <- m.fit.jags.coda[,"a"]
b.sim <- m.fit.jags.coda[,"b"]

par(mfrow=c(1,2), mar=c(6.1, 3.1, 4.1, 2.1), xpd=TRUE)
# plot for intercept
truehist(a.sim, prob=TRUE, col="lavender", xlab=expression(a))
lines(density(a.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.fixed$"(Intercept)",lwd=2)

# plot for slope
truehist(b.sim, prob=TRUE, col="lavender", xlab=expression(b))
lines(density(b.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.fixed$"x",lwd=2)
legend("topright",c("MCMC: JAGS","INLA"), inset=c(-0.18,0), 
       lty=c(3,1),lwd=c(2,2),col=c(2,1),cex=0.7,bty="n")
```

### JAGS: several chains

```{r results = "hold"}

inits_4chain <- list(
  list(a=0, b = 0, .RNG.name = "base::Super-Duper", .RNG.seed = 12345),
  list(a=0, b = 0, .RNG.name = "base::Super-Duper", .RNG.seed = 123456),
  list(a=0, b = 0, .RNG.name = "base::Super-Duper", .RNG.seed = 1234567),
  list(a=0, b = 0, .RNG.name = "base::Super-Duper", .RNG.seed = 12345678))

# model initialisation
model.jags <- jags.model(
  file = "MiceModel.txt",
  data = mice_data,
  inits = inits_4chain,
  n.chains = 4,
  n.adapt = 4000
)

# burn-in
update(model.jags, n.iter = 4000)

# sampling/monitoring
fit.jags.coda <- coda.samples(
  model = model.jags,
  variable.names = c("a", "b"),
  n.iter = 10000,
  thin = 10
)

summary(fit.jags.coda)
plot(fit.jags.coda)

# store samples for each parameter from the chain into separate objects
m.fit.jags.coda <- as.matrix(fit.jags.coda)
a.sim <- m.fit.jags.coda[,"a"]
b.sim <- m.fit.jags.coda[,"b"]

par(mfrow=c(1,2), mar=c(6.1, 3.1, 4.1, 2.1), xpd=TRUE)
# plot for intercept
truehist(a.sim, prob=TRUE, col="lavender", xlab=expression(a))
lines(density(a.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.fixed$"(Intercept)",lwd=2)

# plot for slope
truehist(b.sim, prob=TRUE, col="lavender", xlab=expression(b))
lines(density(b.sim),lty=3,lwd=3, col=2)
lines(inla.output$marginals.fixed$"x",lwd=2)
legend("topright",c("MCMC: JAGS","INLA"), inset=c(-0.2,0), 
       lty=c(3,1),lwd=c(2,2),col=c(2,1),cex=0.7,bty="n")


```

Classical logistic regression model

```{r}

df <- data.frame("dosage" = x, "numtot" = n, "prop" = y/n)

# Centering dosages
df$dosage_centered <- df$dosage - mean(df$dosage)

glm.model <- glm(prop ~ dosage_centered, df, family= "binomial", weight= n)

summary(glm.model)$coef
```

\bigskip
Conclusion/Comparison:  

The estimates for the intercept a and slope b from the JAGS model
are actually relatively similar compared to the classical
logistic regression with the function glm().
Since we estimated the distribution of the slope
and intercept, we can even obtain quantiles
of a and b, which is not possible with the classical approach.
Furthermore, we have a quantification of the computational MCMC
error with the naive se and time-series se, which do not exist in the
classical framework.
Finally, we can see the fundamental differences between the classical
and Bayesian statistics, reflected in the standard error of the estimates in
the classical approach, and the empirical standard deviation in the Bayesian
model.

\newpage

# Exercise 5


Run the logistic regression in JAGS for mice data from Collett (2003, p.71) provided in Table 1,
which you developed in Exercise 4 above.

```{r}
# the covariate values (dose)
x_original <- c(0.0028, 0.0028,0.0056, 0.0112, 0.0225, 0.0450)

# center covariate values
x <- x_original - mean(x_original)

# number of mice deaths
y <- c(26, 9, 21, 9, 6, 1)

# total number of mice
n <- c(28, 12, 40, 40, 40, 40)

## Fit logistic regression
m <- glm(formula = cbind(y, n - y) ~ x, family = binomial)

#create data
data <- matrix(NA, nrow=6, ncol=3)

colnames(data) <- list("x", "y", "n")
data[,1] <- x
data[,2] <- y
data[,3] <- n
mice.data <- list(y=data[,2], x=data[,1],
n=data[,3], N = nrow(data))
```

```{r}

# define parameters
mice.params <- c("a", "b")

# define initial values for 2 chains
inits1 <- list(a=rnorm(1), b=rnorm(1),
               .RNG.name="base::Super-Duper", .RNG.seed=1)

inits2 <- list(a=rnorm(1), b=rnorm(1),
               .RNG.name="base::Wichmann-Hill", .RNG.seed=2)

mice.inits <- list(inits1, inits2)

modelString = " 
# open quote for modelString 
model{ 
for(i in 1:N){
y[i] ~ dbin(p[i],n[i])
logit(p[i]) <- a+b*x[i]
}
a ~ dnorm(0,0.0001)
b ~ dnorm(0,0.0001)
}" # close quote for modelString

writeLines(modelString, con="logistic_regression_JAGS.txt")

# model initiation
mice.rjags <- jags.model(file = "logistic_regression_JAGS.txt",
                         data = mice.data,
                         inits = mice.inits,
                         n.chains = 2,
                         n.adapt = 4000)
N.iter <- 10000
N.thin <- 1
N.burnin <- 4000

# burn-in
update(mice.rjags, n.iter = N.burnin)

# sampling
fit.rjags.coda <- coda.samples(
  model = mice.rjags,
  variable.names = mice.params,
  n.iter = N.iter,
  thin = N.thin)

# store samples for each parameter
a.sample <- fit.rjags.coda[,"a"]
b.sample <- fit.rjags.coda[,"b"]
summary(fit.rjags.coda)

```
## 1
```{r}
##########################
# CODA logistic regression
##########################
# Have we reached convergence in the stationary target (= posterior) distribution 
# with the MCMC, or do we need more simulations (=samples)?

###############
# 1) Rank plots
###############
# To asses convergence to stationarity we implement rank plots
# implement rank plots using posterior samples from each chain to assess mixing and convergence 
# Convergence diagnostics do not guarantee convergence; they only provide evidence against non-convergence. 
# False negatives (type II errors) are a significant risk, leading to potentially invalid inferences.

library(bayesplot)

# Use JAGS model with several chains, output 'fit.jags.coda' from above
# convert output to format that can be used with bayesplot
rank_data <- as.mcmc.list(fit.rjags.coda)

# Rank plots for each chain 
bayesplot::mcmc_rank_hist(rank_data, pars = c("a", "b"))
```
Rank plots show how well multiple chains of MCMC simulation are mixing and exploring the posterior distribution. 
Mixing well? If the histograms are uniformly distributed for each chain, we gave good mixing. Which means that the chains are exploring the posterior distribution well. 

- **Uniform Distributions:** indicates good mixing and that chains are exploring the posterior distribution well.
- **Non-uniform Distributions:** may suggest issues with convergence or exploration of the posterior space.

We can see here that the chains are not quite uniformly distributed. Observed non-uniform distributions in the rank plots for parameters a and b suggest potential issues. This might indicate that some chains are exploring different regions of the parameter space, or there are areas that are underexplored, leading to possible convergence issues.

## 2
```{r}
###################################
# 2) Formal convergence diagnostics
###################################
library(coda)

# a) 
heidel.diag(fit.rjags.coda)
```
Heidelberger & Welch diagnostic test check stationarity and the precision of the estimates (halfwidth test).

- **Stationarity Test:** All parameters passed the stationarity test in the first set, indicating that the chains are stable over iterations. However, in the second set, parameter a failed, suggesting that not all chains may have reached a stable state across all simulations.

- **Halfwidth Test:** For those parameters and simulations where the stationarity test was passed, the halfwidth test also passed, indicating satisfactory precision of the estimates. The failure of parameter a in the second set to pass the stationarity test implies that the halfwidth test could not be reliably evaluated.
```{r}
### Raftery & Lewis Diagnostics ### ----------------------------------
# General idea: Non-parametric approach, based on precision of quantile estimation 
# of the target distribution. Estimates the number of samples needed to estimate
# quantiles of the posterior distribution with a certain precision.

# b)
raftery.diag(fit.rjags.coda)
```
Dependence factor measures the degree to which autocorrelation within the chain inflates the required sample size compared to what would be needed if the samples were independent. Higher values indicate more autocorrelation and thus less efficient sampling. For instance, parameter b in the second result set had a dependence factor of 27.00, suggesting significant autocorrelation.
```{r}
### Geweke Diagnostics ### -------------------------------------------
# General idea: If MCMC has reached convergence, moments are constant in "time".
# Hence, compare mean from first part of MCMC (without burnin!) to mean of last
# part from MCMC with a Z-test.

# c) 
geweke.diag(fit.rjags.coda)
```
geweke.diag shows that the z-scores are not larger than 1.96. 
The z-scores for a and b are within the bound. This suggests that there is no significant difference between the early and late segments of the chain. So we have of convergence to stationarity. So a and b likely drawn from a stationary distributions.


```{r}
geweke.plot(fit.rjags.coda)
```
However, now we see in the visual observation that we have outliers. 
chain 1: the plots show that the some values are outside of the upper bound  (1.96). 
chain2: the plot show that the some values are outside of the lower bound  (-1.96). 

```{r}
### Gelman-Rubin Diagnostic ### ----------------------------
# General idea: Run multiple MCMC (here: 4) and compare the within-chain variance
# to the total variance in all chains ( ANOVA). The "potential scale reduction
# factors" are computed based on normal approximations.
# Chains mix well and converge, if R -> 1.

# Practical implications: If the upper limit of the CI for the the potential scale 
# reduction factor is close to 1, then approximate convergence in the posterior
# distribution is diagnosed.

# (Autoburnin = TRUE: First half of samples is not considered).
# (Assumption: Stationary distribution is normal (if normality assumption
# is violated, a transformation of the data can be applied)).

# d)
gelman.diag(fit.rjags.coda)
```
The upper limit of the CI is close to 1. So we have proper convergence. 



```{r}
gelman.plot(fit.rjags.coda, autoburnin=TRUE)
```
Figures show that the shrink factor R decreases to 1 with increasing number of
iterations. 



```{r}
#e) 

library(stableGR)
stable.GR(fit.rjags.coda)
```

psrf is the potential scale reduction factor. That is the R ans these are smaller than 1.01.


## 3
```{r}
# Modify the original n.burnin, n.iter and n.thin parameters as suggested by the
# convergence diagnostics and re-run the MCMC simulation. Do the new results differ
# much from those in the first run?
# Adjusted model initiation

mice.rjags <- jags.model(file = "logistic_regression_JAGS.txt",
data = mice.data,
inits = mice.inits,
n.chains = 2,
n.adapt = 6000) # increased burn-in
N.iter <- 20000 # increased iterations
N.thin <- 5 # increased thinning

# Re-run sampling
update(mice.rjags, n.iter = 6000) # new burn-in
fit.rjags.coda_adjusted <- coda.samples(
model = mice.rjags,
variable.names = mice.params,
n.iter = N.iter,
thin = N.thin)
summary(fit.rjags.coda_adjusted)

# Original fit summary
original_summary <- summary(fit.rjags.coda)
# Adjusted fit summary after changing n.burnin, n.iter, n.thin
adjusted_summary <- summary(fit.rjags.coda_adjusted)
# Compare summaries
knitr::kable(list(
Original = original_summary$statistics,
Adjusted = adjusted_summary$statistics
))
# Plotting trace plots for comparison
par(mfrow=c(2, 2))
plot(fit.rjags.coda, main="Original Run")
plot(fit.rjags.coda_adjusted, main="Adjusted Run")

# Check effective sample size changes
original_ess <- effectiveSize(fit.rjags.coda)
adjusted_ess <- effectiveSize(fit.rjags.coda_adjusted)
# Display ESS comparison
knitr::kable(list(
Original_ESS = original_ess,
Adjusted_ESS = adjusted_ess
))
```

The results do differ. The effective sample sizes (ESS) have increased in our adjusted MCMC simulation.
Higher ESS values suggest that the chain is sampling more effectively from the posterior distribution, providing
more reliable estimates and reducing the autocorrelation among sampled values. With higher ESS we
also have:

- more precise estimates of the mean, variance, and other moments of the posterior distribution.
- Improved accuracy of quantile estimates.
- Reduced Monte Carlo error.





\newpage

# Exercise 6

Run the code from the previous exercise with mice data with only one chain monitoring beta under the following two conditions:

## 1

After an adaptation phase of 1000 and a burn-in of 4000 draw a sample of 1000
observations in one chain with thinning set to 1.

\hrulefill

\bigskip

```{r }
set.seed(44566)

inits_1chain <- list(a = 0, b = 0, .RNG.name = "base::Wichmann-Hill", .RNG.seed = 123456)

# model initiation
model.jags <- jags.model(
  file = "MiceModel.txt",
  data = mice_data,
  inits = inits_1chain,
  n.chains = 1,
  n.adapt = 1000)

# burn-in
update(model.jags, n.iter = 4000)

# sampling
fit.jags.coda_1 <- coda.samples(
  model = model.jags,
  variable.names = "b",
  n.iter = 1000,
  thin = 1)

#summary(fit.jags.coda)
par(mfrow=c(1,2), mar=c(10,2,4,3))
plot(fit.jags.coda_1)

```
\bigskip

## 2

After an adaptation phase of 1000 and a burn-in of 4000 draw a sample of 10000 observations in one chain with thinning set to 10.

\hrulefill

\bigskip

```{r}
set.seed(44566)

# sampling
fit.jags.coda_2 <- coda.samples(
  model = model.jags,
  variable.names = "b",
  n.iter = 10000,
  thin = 10)

#summary(fit.jags.coda)
par(mfrow=c(1,2), mar=c(10,2,4,3))
plot(fit.jags.coda_2)
```


## (a) 

For which of the above conditions the ESS estimates will be larger and why?

\hrulefill

\bigskip

Without thinning, the samples are dependent on preceding samples since the Markov Chain is kept intact. With thinning, only every x'th sample is chosen, so this dependence is disrupted, and autocorrelation is reduced. Which lower autocorrelation, the resulting ESS is higher.

\bigskip

## (b) 

To check your answer: Apply both the 05ess.R code and the function effectiveSize from the coda package. Compare the ESS estimates with those obtained with the n.eff function from package stableGR (Vats and Knudson, 2021). Please report your findings.

\hrulefill

\bigskip

```{r}

ess_t1 <-   ess(fit.jags.coda_1[[1]], 1000)
ess_t10 <- ess(fit.jags.coda_2[[1]], 1000)

coda_ess_t1 <- as.numeric(effectiveSize(fit.jags.coda_1))
coda_ess_t10 <- as.numeric(effectiveSize(fit.jags.coda_2))

stableGR_ess_t1 <- as.numeric(n.eff(fit.jags.coda_1)$n.eff)
stableGR_ess_t10 <- as.numeric(n.eff(fit.jags.coda_2)$n.eff)

```


\begin{table}[h!]
\begin{tabular}{lccc}
\hline
Setting & custom function & coda::effectiveSize & stableGR::n.eff \\ \hline
n.iter = 1000, thin = 1 & `r round(ess_t1,2)` & `r round(coda_ess_t1,2)` & `r round(stableGR_ess_t1,2)`  \\ \hline
n.iter = 10000, thin = 10 & `r round(ess_t10,3)` & `r round(coda_ess_t10,2)` & `r round(stableGR_ess_t10,3)`  \\ \hline
\end{tabular}
\end{table}


Indeed, the effective sample sizes are higher with thinning.
The different packages don't all return exactly the same values, but they're in comparable ranges.

