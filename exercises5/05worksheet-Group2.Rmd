---
title: "Worksheet 05 Group 2"
author:
- "Andrea Staub"
- "Emanuel Mauch"
- "Holly Vuarnoz"
- "Jan Hohenheim"
- "Sophie Haldemann"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# Set chunk options here 
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(glue)
library(rjags)
library(INLA)
library(MASS)
library(ggplot2)
library(patchwork)
library(coda)
library(stableGR)
source("05ess.r")
```






\newpage

# Exercise 6

Run the code from the previous exercise with mice data with only one chain monitoring beta under the following two conditions:

## 1

After an adaptation phase of 1000 and a burn-in of 4000 draw a sample of 1000
observations in one chain with thinning set to 1.

\hrulefill

\bigskip

```{r }
set.seed(44566)

wb_inits <- wb_inits <- list(a = 0, b = 0, .RNG.name = "base::Wichmann-Hill", .RNG.seed = 123456)

# model initiation
model.jags <- jags.model(
  file = "TempModel.txt",
  data = wb_data,
  inits = wb_inits,
  n.chains = 1,
  n.adapt = 1000)

# burn-in
update(model.jags, n.iter = 4000)

# sampling
fit.jags.coda_1 <- coda.samples(
  model = model.jags,
  variable.names = "b",
  n.iter = 1000,
  thin = 1)

#summary(fit.jags.coda)
par(mfrow=c(1,2), mar=c(10,2,4,3))
plot(fit.jags.coda_1)

```
\bigskip

## 2

After an adaptation phase of 1000 and a burn-in of 4000 draw a sample of 10000 observations in one chain with thinning set to 10.

\hrulefill

\bigskip

```{r}
set.seed(44566)

# sampling
fit.jags.coda_2 <- coda.samples(
  model = model.jags,
  variable.names = "b",
  n.iter = 10000,
  thin = 10)

#summary(fit.jags.coda)
par(mfrow=c(1,2), mar=c(10,2,4,3))
plot(fit.jags.coda_2)
```


## (a) 

For which of the above conditions the ESS estimates will be larger and why?

\hrulefill

\bigskip

Without thinning, the samples are dependent on preceding samples since the Markov Chain is kept intact. With thinning, only every x'th sample is chosen, so this dependence is disrupted, and autocorrelation is reduced. Which lower autocorrelation, the resulting ESS is higher.

\bigskip

## (b) 

To check your answer: Apply both the 05ess.R code and the function effectiveSize from the coda package. Compare the ESS estimates with those obtained with the n.eff function from package stableGR (Vats and Knudson, 2021). Please report your findings.

\hrulefill

\bigskip

```{r}

ess_t1 <-   ess(fit.jags.coda_1[[1]], 1000)
ess_t10 <- ess(fit.jags.coda_2[[1]], 1000)

coda_ess_t1 <- as.numeric(effectiveSize(fit.jags.coda_1))
coda_ess_t10 <- as.numeric(effectiveSize(fit.jags.coda_2))

stableGR_ess_t1 <- as.numeric(n.eff(fit.jags.coda_1)$n.eff)
stableGR_ess_t10 <- as.numeric(n.eff(fit.jags.coda_2)$n.eff)

```


\begin{table}[h!]
\begin{tabular}{lccc}
\hline
Setting & custom function & coda::effectiveSize & stableGR::n.eff \\ \hline
n.iter = 1000, thin = 1 & `r round(ess_t1,2)` & `r round(coda_ess_t1,2)` & `r round(stableGR_ess_t1,2)`  \\ \hline
n.iter = 10000, thin = 10 & `r round(ess_t10,3)` & `r round(coda_ess_t10,2)` & `r round(stableGR_ess_t10,3)`  \\ \hline
\end{tabular}
\end{table}


Indeed, the effective sample sizes are higher with thinning.
The different packages don't all return exactly the same values, but they're in comparable ranges.


